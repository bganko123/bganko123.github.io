# An overview of TSNE

1. TOC
{:toc}

## What is the point of t-SNE

t-distributed stochastic neighbor embedding or t-SNE (I'll stick to the latter) 
is a handy way of representing high dimensional data by gving each data point a location on a two dimensional map. 
It can be expanded to three dimensions but this adds complexity. 
In effect, high dimiensional data points that are similar will be shown close to each other in the two 
dimensional plot and dissimilar ones will be further away.
The point of this is to give you an intuitive view of high dimensional data [^1]

## What does it do

t-SNE makes a probability distribution over pairs of the highdimensional objects and then  makes a new probability distribution in the lower dimensional map 
and attempts to minimise the distance using some form of cost fucntion. The process can be broken down into three broad steps [^2]:
1. Step one is to measure the similarities in high dimesional space. A gaussian distribution is centred over each data point in the high dimensional space. Then the density of all the points under that gaussian is measured. This results in a set of probabilities for each one and these probabilities are proportional to the similarities between points. So for two points,x1 and x2, if the probabilities are equal or close, they have local similarities in the high dimensional space. The way in whcih the gaussian is applied can be varied by changing the perpelexity, this determines the size of the distribution and therefore the number of points that fall within each one.
2. Similar to step one, but instead a t-distribution (hence the t in t-SNE) is used with one degree of freedom. This provides a set of probabilities in the lower dimensional space. 
3. Finally, it is ideal for the probabilities in the high dimensional space to refelect the probabilities in the low dimensional space as closely as possible. This is done using a cost function known as the Kullback-Liebler divergence (KL). It minimises the distances between the two probabilities so that the two map structures will be similar.

## What should t-SNE be used for

t-SNE can be used in a wide variety of setting - computer science, climate research, medicine - any scenarios were high dimensional data is produced and requires analysis. t-SNE is good at showing seperation in data points that can't otherwise be seen due to the high dimensionality. Similar points will appear in identifiable clusters that validates there similarities [^3].

## t-SNE Implementation using scikit-learn

There are a number of libraries to implement t-SNE on a data set. A popular one available in python is scikit-learn. The following example is from a post by DANIEL MÃœLLER-KOMOROWSKA. He implements a learning model in the MNIST data set of hand written numbers. He uses the following code example to illustrate the ease of use:

from sklearn.manifold import TSNE
import pandas as pd
import seaborn as sns
 
# We want to get TSNE embedding with 2 dimensions
n_components = 2
tsne = TSNE(n_components)
tsne_result = tsne.fit_transform(X)
tsne_result.shape
# (1000, 2)
# Two dimensions for each of our images
 
# Plot the result of our TSNE with the label color coded
# A lot of the stuff here is about making the plot look pretty and not TSNE
tsne_result_df = pd.DataFrame({'tsne_1': tsne_result[:,0], 'tsne_2': tsne_result[:,1], 'label': y})
fig, ax = plt.subplots(1)
sns.scatterplot(x='tsne_1', y='tsne_2', hue='label', data=tsne_result_df, ax=ax,s=120)
lim = (tsne_result.min()-5, tsne_result.max()+5)
ax.set_xlim(lim)
ax.set_ylim(lim)
ax.set_aspect('equal')
ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)


[^1]: https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding
[^2]: https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1
[^3]: https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/
 
